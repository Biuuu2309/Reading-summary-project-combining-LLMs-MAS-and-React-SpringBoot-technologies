{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd80eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e962ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-cloud-vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cafbb9a",
   "metadata": {},
   "source": [
    "----------------------------------------------------TEST----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6b2540",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from google.cloud import vision\n",
    "\n",
    "# ‚úÖ Thi·∫øt l·∫≠p credentials ƒë√∫ng file JSON c·ªßa b·∫°n\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/drive/MyDrive/credentials/projectsummarize-ee4dd5a1f9e8.json\"\n",
    "\n",
    "# üîπ Kh·ªüi t·∫°o client\n",
    "client = vision.ImageAnnotatorClient()\n",
    "\n",
    "# üîπ H√†m OCR\n",
    "def google_vision_ocr(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        content = image_file.read()\n",
    "    image = vision.Image(content=content)\n",
    "    response = client.document_text_detection(image=image)\n",
    "    return response.full_text_annotation.text\n",
    "\n",
    "# ‚úÖ Test OCR v·ªõi ·∫£nh c·ªßa b·∫°n\n",
    "image_path = \"/content/drive/MyDrive/images/Screenshot 2025-10-04 154203.png\"  # thay n·∫øu kh√°c\n",
    "text = google_vision_ocr(image_path)\n",
    "\n",
    "print(\"===== K·∫øt qu·∫£ OCR =====\")\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150992ba",
   "metadata": {},
   "source": [
    "----------------------------------------------------START----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c7ee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import vision\n",
    "\n",
    "# ƒê·∫£m b·∫£o Drive ƒë√£ ƒë∆∞·ª£c mount v√† credentials ƒë√£ set\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/drive/MyDrive/credentials/projectsummarize-ee4dd5a1f9e8.json\"\n",
    "\n",
    "client = vision.ImageAnnotatorClient()\n",
    "\n",
    "def google_vision_ocr(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        content = image_file.read()\n",
    "    image = vision.Image(content=content)\n",
    "    response = client.document_text_detection(image=image)\n",
    "    return response.full_text_annotation.text\n",
    "\n",
    "# üîπ Th∆∞ m·ª•c ch·ª©a ·∫£nh\n",
    "image_folder = \"/content/drive/MyDrive/images5/\"\n",
    "output_file = \"/content/drive/MyDrive/export_to_text/vision_text5.txt\"\n",
    "\n",
    "# üîπ M·ªü file ƒë·ªÉ ghi (ch·∫ø ƒë·ªô 'a' = append th√™m n·ªôi dung m·ªõi)\n",
    "with open(output_file, \"a\", encoding=\"utf-8\") as f:\n",
    "    for filename in os.listdir(image_folder):\n",
    "        if filename.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tiff\")):\n",
    "            image_path = os.path.join(image_folder, filename)\n",
    "            print(f\"üîç ƒêang OCR ·∫£nh: {filename}\")\n",
    "            text = google_vision_ocr(image_path)\n",
    "\n",
    "            # Ghi v√†o file v·ªõi ti√™u ƒë·ªÅ ·∫£nh\n",
    "            f.write(f\"\\n===== ·∫¢NH: {filename} =====\\n\")\n",
    "            f.write(text + \"\\n\")\n",
    "\n",
    "print(\"‚úÖ Ho√†n th√†nh OCR to√†n b·ªô ·∫£nh v√† l∆∞u v√†o vision_text2.txt!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e334e925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# Colab script: S·ª≠a d·∫•u + s·ª≠a l·ªói nh·∫π b·∫±ng t·ª´ ƒëi·ªÉn (offline)\n",
    "# =====================\n",
    "\n",
    "# Y√™u c·∫ßu: ƒë√£ mount Drive tr∆∞·ªõc ƒë√≥\n",
    "\n",
    "import json\n",
    "import unicodedata\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------------------\n",
    "# C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n\n",
    "# ---------------------\n",
    "DICT_PATH = \"/content/drive/MyDrive/vietnamese_dict.txt\"   # file JSONL b·∫°n ƒë√£ c√≥\n",
    "INPUT_FILE = \"/content/drive/MyDrive/export_to_text/vision_text5.txt\"\n",
    "OUTPUT_FILE = \"/content/drive/MyDrive/export_to_text/vision_text_fixed5.txt\"\n",
    "\n",
    "# ---------------------\n",
    "# H√†m ti·ªán √≠ch: remove accents (chu·∫©n ho√° kh√¥ng d·∫•u)\n",
    "# ---------------------\n",
    "def remove_accents(s: str) -> str:\n",
    "    # chu·∫©n ho√° NFD r·ªìi lo·∫°i b·ªè c√°c k√Ω t·ª± combining\n",
    "    s = unicodedata.normalize('NFD', s)\n",
    "    s = ''.join(ch for ch in s if unicodedata.category(ch) != 'Mn')\n",
    "    # chuy·ªÉn m·ªôt s·ªë k√Ω t·ª± ƒë·∫∑c bi·ªát v·ªÅ chu·∫©n ascii\n",
    "    s = s.replace('ƒë', 'd').replace('ƒê', 'D')\n",
    "    return unicodedata.normalize('NFC', s)\n",
    "\n",
    "# ---------------------\n",
    "# H√†m Levenshtein v·ªõi ng∆∞·ª°ng d·ª´ng s·ªõm (early stop)\n",
    "# ---------------------\n",
    "def levenshtein_with_max(a: str, b: str, max_dist: int):\n",
    "    # returns distance if <= max_dist, else returns a value > max_dist\n",
    "    # optimized: use two-row DP\n",
    "    if abs(len(a) - len(b)) > max_dist:\n",
    "        return max_dist + 1\n",
    "    if len(a) < len(b):\n",
    "        a, b = b, a\n",
    "    previous = list(range(len(b) + 1))\n",
    "    for i, ca in enumerate(a, start=1):\n",
    "        current = [i] + [0] * len(b)\n",
    "        # compute current row with pruning\n",
    "        min_in_row = current[0]\n",
    "        for j, cb in enumerate(b, start=1):\n",
    "            insert_cost = current[j-1] + 1\n",
    "            delete_cost = previous[j] + 1\n",
    "            replace_cost = previous[j-1] + (0 if ca == cb else 1)\n",
    "            current[j] = min(insert_cost, delete_cost, replace_cost)\n",
    "            if current[j] < min_in_row:\n",
    "                min_in_row = current[j]\n",
    "        if min_in_row > max_dist:\n",
    "            return max_dist + 1\n",
    "        previous = current\n",
    "    return previous[-1]\n",
    "\n",
    "# ---------------------\n",
    "# Load t·ª´ ƒëi·ªÉn JSONL -> t·∫≠p c√°c t·ª´ h·ª£p l·ªá + map kh√¥ng d·∫•u -> list t·ª´\n",
    "# ---------------------\n",
    "vietnamese_words = set()\n",
    "deaccent_map = defaultdict(list)\n",
    "\n",
    "print(\"‚è≥ ƒêang load t·ª´ ƒëi·ªÉn t·ª´:\", DICT_PATH)\n",
    "with open(DICT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            item = json.loads(line)\n",
    "            word = item.get(\"text\", \"\").strip()\n",
    "            if not word:\n",
    "                continue\n",
    "            # chu·∫©n ho√°: ch·ªØ th∆∞·ªùng\n",
    "            key = word.lower()\n",
    "            # ch·ªâ gi·ªØ n·∫øu c√≥ ch·ªØ (c√≥ th·ªÉ ch·ª©a space cho c·ª•m t·ª´)\n",
    "            # lo·∫°i b·ªè nh·ªØng d√≤ng c√≥ k√≠ t·ª± l·∫° ·ªü ƒë·∫ßu\n",
    "            if any(ch.isalpha() for ch in key):\n",
    "                vietnamese_words.add(key)\n",
    "                deacc = remove_accents(key).lower()\n",
    "                deaccent_map[deacc].append(key)\n",
    "        except Exception as e:\n",
    "            # b·ªè qua d√≤ng l·ªói\n",
    "            continue\n",
    "\n",
    "print(\"‚úÖ ƒê√£ load t·ª´ ƒëi·ªÉn. S·ªë t·ª´ (c·ª•m t·ª´) trong b·ªô t·ª´ ƒëi·ªÉn:\", len(vietnamese_words))\n",
    "print(\"V√≠ d·ª• 20 t·ª´:\", list(vietnamese_words)[:20])\n",
    "\n",
    "# ---------------------\n",
    "# Tokenize ƒë∆°n gi·∫£n (gi·ªØ punctuation ƒë·ªÉ ph·ª•c h·ªìi)\n",
    "# ---------------------\n",
    "token_split_re = re.compile(r\"(\\w+|[^\\w\\s])\", flags=re.UNICODE)\n",
    "\n",
    "def smart_tokenize(line: str):\n",
    "    # tr·∫£ v·ªÅ list token, gi·ªØ nguy√™n d·∫•u c√¢u l√† token ri√™ng\n",
    "    return token_split_re.findall(line)\n",
    "\n",
    "def is_number_only(s: str):\n",
    "    s_stripped = s.strip()\n",
    "    # n·∫øu ch·ªâ c√≥ s·ªë ho·∫∑c ch·ªâ ch·ª©a s·ªë+punct\n",
    "    return bool(re.fullmatch(r\"[\\d\\W]+\", s_stripped)) and any(ch.isdigit() for ch in s_stripped)\n",
    "\n",
    "# ---------------------\n",
    "# H√†m s·ª≠a 1 token b·∫±ng t·ª´ ƒëi·ªÉn\n",
    "# ---------------------\n",
    "def correct_token(token: str, max_lev=2):\n",
    "    \"\"\"\n",
    "    token: token nguy√™n b·∫£n (c√≥ th·ªÉ k√®m hoa th∆∞·ªùng)\n",
    "    tr·∫£ v·ªÅ token ƒë√£ s·ª≠a (gi·ªØ nguy√™n ki·ªÉu hoa)\n",
    "    logic:\n",
    "      1. n·∫øu token trong dict -> gi·ªØ\n",
    "      2. th·ª≠ t√¨m trong deaccent_map (kh·ªõp kh√¥ng d·∫•u)\n",
    "      3. t√¨m t·ª´ g·∫ßn nh·∫•t b·∫±ng levenshtein trong dictionary nh∆∞ng ch·ªâ x√©t t·ª´ c√≥ ƒë·ªô d√†i t∆∞∆°ng ƒë∆∞∆°ng\n",
    "    \"\"\"\n",
    "    orig = token\n",
    "    # gi·ªØ nguy√™n n·∫øu token l√† punctuation\n",
    "    if not any(ch.isalpha() for ch in token):\n",
    "        return token\n",
    "\n",
    "    # t√°ch prefix/suffix punctuation (v√≠ d·ª•: \"Em,\" -> 'Em' v√† ',')\n",
    "    prefix = re.match(r'^[^\\w]*', token).group(0) if re.match(r'^[^\\w]*', token) else ''\n",
    "    suffix = re.search(r'[^\\w]*$', token).group(0) if re.search(r'[^\\w]*$', token) else ''\n",
    "    core = token[len(prefix): len(token)-len(suffix) if suffix else None]\n",
    "\n",
    "    if not core:\n",
    "        return token\n",
    "\n",
    "    # remember capitalization\n",
    "    is_title = core.istitle()\n",
    "    is_upper = core.isupper()\n",
    "\n",
    "    core_low = core.lower()\n",
    "\n",
    "    # 1) exact match\n",
    "    if core_low in vietnamese_words:\n",
    "        corrected = core_low\n",
    "    else:\n",
    "        # 2) match via deaccent map\n",
    "        de = remove_accents(core_low)\n",
    "        candidates = deaccent_map.get(de, [])\n",
    "        if candidates:\n",
    "            # choose candidate with minimal levenshtein to core_low (with limit)\n",
    "            best = None\n",
    "            bestd = max_lev + 10\n",
    "            for c in candidates:\n",
    "                d = levenshtein_with_max(core_low, c, max_lev)\n",
    "                if d <= max_lev and d < bestd:\n",
    "                    best = c\n",
    "                    bestd = d\n",
    "            if best:\n",
    "                corrected = best\n",
    "            else:\n",
    "                # if none within max_lev, choose first candidate as fallback\n",
    "                corrected = candidates[0]\n",
    "        else:\n",
    "            # 3) fallback: scan dictionary for nearby words (expensive but limited by length)\n",
    "            best = None\n",
    "            bestd = max_lev + 10\n",
    "            L = len(core_low)\n",
    "            # iterate through words of similar length only to reduce cost\n",
    "            # choose lengths L-1, L, L+1\n",
    "            for w in vietnamese_words:\n",
    "                if abs(len(w) - L) > 1:\n",
    "                    continue\n",
    "                d = levenshtein_with_max(core_low, w, max_lev)\n",
    "                if d <= max_lev and d < bestd:\n",
    "                    best = w\n",
    "                    bestd = d\n",
    "                    if bestd == 0:\n",
    "                        break\n",
    "            if best:\n",
    "                corrected = best\n",
    "            else:\n",
    "                # kh√¥ng t√¨m ƒë∆∞·ª£c -> gi·ªØ nguy√™n\n",
    "                corrected = core_low\n",
    "\n",
    "    # restore capitalization similar to original\n",
    "    if is_upper:\n",
    "        out = corrected.upper()\n",
    "    elif is_title:\n",
    "        out = corrected.capitalize()\n",
    "    else:\n",
    "        out = corrected\n",
    "\n",
    "    return prefix + out + suffix\n",
    "\n",
    "# ---------------------\n",
    "# X·ª≠ l√Ω to√†n file\n",
    "# ---------------------\n",
    "print(\"‚è≥ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω file:\", INPUT_FILE)\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as fin, \\\n",
    "     open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for raw_line in tqdm(fin):\n",
    "        line = raw_line.rstrip(\"\\n\")\n",
    "        stripped = line.strip()\n",
    "\n",
    "        # Gi·ªØ nguy√™n c√°c d√≤ng ƒë·ªãnh d·∫°ng ·∫£nh\n",
    "        if stripped.startswith(\"=====\") and \"·∫¢NH\" in stripped:\n",
    "            fout.write(line + \"\\n\")\n",
    "            continue\n",
    "\n",
    "        # B·ªè c√°c d√≤ng ch·ªâ to√†n s·ªë / k√Ω t·ª± r√°c\n",
    "        if is_number_only(stripped):\n",
    "            # b·ªè ho√†n to√†n (ho·∫∑c c√≥ th·ªÉ ghi newline n·∫øu mu·ªën gi·ªØ v·ªã tr√≠)\n",
    "            # ch√∫ng ta ghi 1 d√≤ng r·ªóng ƒë·ªÉ gi·ªØ ƒë√¥i ch√∫t format\n",
    "            fout.write(\"\\n\")\n",
    "            continue\n",
    "\n",
    "        if stripped == \"\":\n",
    "            fout.write(\"\\n\")\n",
    "            continue\n",
    "\n",
    "        # Tokenize v√† s·ª≠a t·ª´ng token\n",
    "        tokens = smart_tokenize(line)\n",
    "        corrected_tokens = [correct_token(t) for t in tokens]\n",
    "        new_line = \"\".join(\n",
    "            # smart join: n·∫øu token l√† word and next token is word, insert space\n",
    "            # but tokenization keeps words and punctuation separate; we'll rebuild with spaces when needed\n",
    "            corrected_tokens[i] + (\"\" if (i+1 < len(corrected_tokens) and re.match(r'[^\\w\\s]', corrected_tokens[i+1])) else \" \")\n",
    "            for i in range(len(corrected_tokens))\n",
    "        ).rstrip()\n",
    "\n",
    "        fout.write(new_line + \"\\n\")\n",
    "\n",
    "print(\"‚úÖ Ho√†n t·∫•t! File k·∫øt qu·∫£ l∆∞u t·∫°i:\", OUTPUT_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac85a0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "dict_path = \"/content/drive/MyDrive/vietnamese_dict.txt\"  # ‚úÖ ƒê∆∞·ªùng d·∫´n b·∫°n cung c·∫•p\n",
    "\n",
    "vietnamese_words = set()\n",
    "with open(dict_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            item = json.loads(line.strip())\n",
    "            word = item.get(\"text\", \"\").strip().lower()\n",
    "            if word and word.replace(\" \", \"\").isalpha():  # Cho ph√©p c·ª•m t·ª´ c√≥ d·∫•u c√°ch\n",
    "                vietnamese_words.add(word)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "print(\"‚úÖ T·ªïng s·ªë t·ª´ trong t·ª´ ƒëi·ªÉn:\", len(vietnamese_words))\n",
    "print(list(vietnamese_words)[:20])  # xem th·ª≠\n",
    "\n",
    "# ==========================================\n",
    "# 3Ô∏è‚É£ H√†m l·ªçc t·ª´ng d√≤ng\n",
    "# ==========================================\n",
    "def clean_line(line):\n",
    "    words = line.split()\n",
    "    cleaned = [w for w in words if w.lower().strip(\".,!?;:()\\\"'\") in vietnamese_words]\n",
    "    return \" \".join(cleaned)\n",
    "\n",
    "# ==========================================\n",
    "# 4Ô∏è‚É£ ƒê·ªçc file OCR g·ªëc ‚Üí t·∫°o file s·∫°ch\n",
    "# ==========================================\n",
    "input_file = \"/content/drive/MyDrive/export_to_text/vision_text_fixed5.txt\"\n",
    "output_file = \"/content/drive/MyDrive/export_to_text/vision_text_clean5.txt\"\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as fin, \\\n",
    "     open(output_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for line in fin:\n",
    "        cleaned = clean_line(line)\n",
    "        fout.write(cleaned + \"\\n\")\n",
    "\n",
    "print(\"‚úÖ ƒê√£ l·ªçc xong! File m·ªõi t·∫°i:\", output_file)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
